---
title: Regression 1
format: revealjs
editor: source
execute:
  echo: true
html:
  code-fold: true
  code-summary: Show the code
---

## Goals for the slide deck

Example using `brms` to help with nomenclature and procedure. Begin to think about how we can "feed" our model for insight and profit

## Specify the model

We want to understand why i people differ on the mean of the DV.

$$ y_i \sim Normal( \mu_i , \sigma )\ $$ $$\mu_i = \beta_1  X_i $$

-   You also have a parameter that is estimated called sigma, which in R lm output is hidden under Residual Standard Error.
-   This way writing a model is more explicit about 1. your data generating process and 2. what parameters you are modeling.

## Specify the model

-   We will use this same nomenclature to describe our priors on each of the parameters we are modeling. For example, we are estimating $\beta$ and $\sigma$, and thus we need two priors.

$$ y_i \sim Normal( \mu_i , \sigma )\ $$ $$\mu_i = \beta_1  X_i $$

$$\beta_1 \sim Normal(0, 5)\ $$ $$\sigma \sim HalfCauchy(0,10) $$

## priors

In general we want to:

1.  Center our priors around a null effect
2.  Have dispersion (our accuracy metric) be a balance between not too large and not too little. This choice requires though, whereas centering a prior around zero requires none.

This also get around the "subjectivty in objective science" arguments against bayes.

## Example

```{r}
#| code-fold: true
library(psychTools)
galton.data <- galton
library(tidyverse)
glimpse(galton.data)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
galton.data %>% 
  ggplot(aes(x = parent, y = child)) +
  geom_jitter(alpha = 1/2) 

```

## Height data

::: columns
::: {.column width="50%"}
```{r}
#| code-fold: true
galton.data %>% 
ggplot(aes(x = child)) +geom_density()

```
:::

::: {.column width="50%"}
```{r}
#| code-fold: true
library(easystats)
report_sample(galton.data)
```
:::
:::

## Specify our model

Likelihood: $$ \text{C_Height}_i \sim Normal( \mu_i , \sigma ) $$ Linear model: $$\mu_i = \beta_0 + \beta_1 ( \text{P_Height}_i - {\overline{\mbox{P_Height}}} ) $$

Priors: $$\beta_0 \sim Normal(68, 5)$$ $$\beta_1 \sim Normal(0, 5)$$ $$\sigma \sim HalfCauchy(0,1)$$

## Centering our predictors

```{r}
galton.data <- galton.data %>% 
  mutate(parent.c = parent - mean(parent))
galton.data 
```

## Prior for intercept

```{r}
#| code-fold: true

  tibble(x = seq(from = 0, to = 100, by = .1)) %>% 
  ggplot(aes(x = x, y = dnorm(x, mean = 68, sd = 5))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +
  labs(title = "mu ~ dnorm(68, 5)",
       y = "density")

```

## Prior for regression coefficent

```{r}
#| code-fold: true
  tibble(x = seq(from = -15, to = 15, by = .1)) %>% 
  ggplot(aes(x = x, y = dnorm(x, mean = 0, sd = 5))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = -15, to = 15, by = 3)) +
  labs(title = "mu ~ dnorm(0, 5)",
       y = "density")
```

## Prior for sigma

```{r}
#| code-fold: true
p.s <- ggplot(data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +
  labs(title = "sigma ~ HalfCauchy(0,1)")
p.s
```

-   We know that variances are going to be positive.

-   What is an upper bound possibility?

## Prior predictive

-   What do our priors say about what our model expects? This is a way to check if our priors our too liberal or conservative.

-   We can take our prior and simulate what they say about our potential results.

-   Our goal is to create an efficient and useful model. One that makes impossible predictions prior to seeing the data isn't too useful.

## Prior predictive

How do we create it? We sample from the priors. Use that to create regression lines (intercept and slope). Then plot.

```{r}
#| code-fold: true
tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = 5)) %>% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %>% 
  mutate(c.height = a + b * (height - mean(galton.data$parent))) %>% 
   ggplot(aes(x = height, y = c.height, group = n)) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(36, 96)) 
```

## Prior predictive

We could do a few things:

1.  Constrain the slope to be positive
2.  Reduce the uncertainty (SDs) in our priors
3.  Leave as is

It really depends on whether the priors are important and/or costly, computationally

## brms syntax

```{r, eval = FALSE}
model.name <- # name your fit
  brm(family = gaussian, # what is your likelihood? 
      Y ~ X, # insert model
      prior = prior, # your priors go here
      data = data,  # your data goes here
      iter = 1000, warmup = 500, chains = 4, cores = 4, # wait for this 
      backend = "cmdstanr", # quicker than alternative Rstan library
      file = "fits/b04.01") # save your samples
```

## brms

You can also set aspects of it separately

```{r, eval = FALSE}

#formulas
brmsformula()
brmsformula(x~y, family = gaussian())
bf()

#priors
set_prior()
set_prior("normal(0, 5)", class = "b", coef = "parent")

```

## brms

```{r}
library(brms)
fit.1 <- 
  brm(family = gaussian,
      child ~ 1 + parent.c,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      backend = "cmdstanr",
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "fit.1")
```

## brms

-   What if you don't know what priors should be in your model?
-   Default priors are usually bad (i.e flat and non-centered)

```{r}
#| code-fold: true
get_prior(family = gaussian,
      child ~ 1 + parent.c,
      data = galton.data)
```

## brms

```{r}
#| code-fold: true
summary(fit.1)
```

## compare with lm

```{r}
#| code-fold: true
summary(lm(child ~ parent.c, data = galton.data))
```

------------------------------------------------------------------------

```{r}
plot(fit.1)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
plot(conditional_effects(fit.1), points = TRUE)

```

## Posterior

-   The posterior is made up of samples (often referred to as draws).

-   Visualize with a distribution

-   Very simplistically, the algorithm tries different potential values. Parameter values that are more consistent with the data will come up more often, much like our coin flipping example

## What is in the brms object?

The posterior!

```{r}
#| code-fold: true
as_draws(fit.1)
```

## What is in the brms object?

```{r}
#| code-fold: true
library(tidybayes)
get_variables(fit.1)

```

## {tidybayes}

-   {tidybayes} is a helpful package to work with your posterior. It consists of a number of helper functions to put your posterior in a format to neatly graph and or compute values. Once we get to more complicated models this will be invaluable.

-   Please take a look at the overview and download the package http://mjskay.github.io/tidybayes/articles/tidy-brms.html

-   {marginaleffects} and {bayestestR} from easystats are two other packages we will routinely use

## posterior samples/draws

1k rows indicate 1k parameter samples (we told the algo to give us 1k). All of these parameters represent *possible* states of the world given our data. Parameters more consistent with the data will appear more often.

```{r}
tidy_draws(fit.1)
```

## spread_draws

Instead of looking at all of our "draws" or samples for all variables we often want to look at just some of them. Spread draws is an oft used function similar to select in {dplyr}

```{r}
fit.1 %>% 
spread_draws(b_Intercept, b_parent.c)
```

## Pairs plot

marginal (i.e., averaged over the other parameters) posteriors and the covariances across draws (e.g., when the algorithm found a high value for bo did it also find a high value for sigma?). You want to see random clouds.

```{r}
#| code-fold: true
pairs(fit.1)
```

## Posterior

-   Once we have the posterior in an easy to use format, we can:

1.  Visualize it. (Distributions, predicted values)
2.  Calculate values from it. (Summary statistics, contrasts, new parameters)

-   Anything we want to accomplish with our model is IN the posterior

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit.1 %>% 
spread_draws(b_Intercept, b_parent.c) %>% 
  ggplot(aes(x = b_parent.c)) +
  stat_dotsinterval()
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit.1 %>% 
spread_draws(b_Intercept, b_parent.c) %>%  
  select(b_parent.c) %>% 
  mean_qi(.width = c(.5, .89, .95, .99))
```

```{r}
#| code-fold: true
fit.1 %>% 
spread_draws(b_Intercept, b_parent.c) %>% 
  select(b_parent.c) %>% 
  mode_hdi(.width = c(.55, .89, .95, .99))
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
  fit.1 %>% 
  spread_draws(b_parent.c) %>% 
  mode_hdi(.width = c(.55, .89, .95, .99)) %>%  
ggplot(aes(y = as.factor(.width), x = b_parent.c, xmin = .lower, xmax = .upper)) + geom_pointinterval() 
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit.1 %>% 
  spread_draws(b_parent.c) %>% 
  ggplot(aes(x = b_parent.c)) +
  stat_halfeye()
```

## prior and posterior plotted together

```{r}
#| code-fold: true
fit.1 %>% 
  spread_draws(b_parent.c) %>% 
  ggplot(aes(x = b_parent.c)) +
  stat_slab() +
  stat_function(data = data.frame(x = c(-10, 10)), aes(x), fun = dnorm, n = 100, args = list(0, 5)) 
```

# Understanding our model

Most often we look at coefficients to interpret our models. This is fine, but it sub-optimal for a number of reasons, especially when we get to more complex models. We need to define the specific empirical quantity that would shed light on our research question. The *estimand* is the object of inquiry---it is the precise quantity about which we marshal data to draw an inference.

Raw parameters may be very difficult to interpret substantively. E.g., logistic. Even odd ratios are hard because odds are un-intuitive.

## Predicted/fitted values

Much like in regular regression, we will use the fitted/predicted/Y-hats values to interpret our models.

```{r}
#| code-fold: true
library(broom)
augment(lm(child ~ parent, galton.data))
```

## Predicted/fitted values

Bayesian analysis also has fitted values, but now we have many samples of parameters rather than just a single estimate for each value.

$$ \hat{Y}_{prediction} = b_o + b_1X $$ If we previously had 928 fitted values. We now have 928 participants \* 1000 samples = 928,000 fitted values to work with. (Though people with the same X have the same yhat so it is effectively less)

## Predicted/fitted values

One reason fitted values are helpful is to showcase uncertainty. That is what our posterior is highlighting: that there is no ONE result, that there are many possible results.

```{r}
#| code-fold: true
ggplot(galton.data, aes(x = child, y = parent)) + 
  geom_point() +
  stat_smooth(method = "lm")
```

The confidence band tells us potential expected values of Y given X (Y\|X)

## Predicted/fitted values

::: columns
::: {.column width="50%"}
If we examine a expected/predicted mean at a certain value across all of our samples we directly compute our uncertainty. In contrast to frequentist where we have to use an ugly equation, which has big assumptions.
:::

::: {.column width="50%"}
```{r}
#| code-fold: true
fit.1 %>% 
  spread_draws(b_Intercept,b_parent.c) %>% 
  select(b_Intercept,b_parent.c) %>% 
  mutate(mu_at_64 = b_Intercept + (b_parent.c * -4.3))
```
:::
:::

## Predicted/fitted values

We can calculate not only the mean but also the dispersion. In lm land we had to use a funky equation to calculate the CI around some predicted value of X. Now we can use samples. It is just counting up where the xx% of samples fall.

```{r}
#| code-fold: true
fit.1 %>% 
  spread_draws(b_Intercept,b_parent.c) %>% 
  select(b_Intercept,b_parent.c) %>% 
  mutate(mu_at_64 = b_Intercept + (b_parent.c * -4.3)) %>% 
  ggplot(aes(x = mu_at_64)) +
  stat_slab() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["C-height at parent 64"])) 

```

## Predicted/fitted values

-   What did we do? We calculated the value of our DV when our predictor = -4.3.

-   How did we do it? We fed our model an X that we were interested in to calculate a Y-hat.

-   We will be using this idea A. LOT. This isn't Bayesian specific. Instead it is a way you should think about all models. It is often VERY useful to use (e.g., getting group values when using dummy's, testing contrasts, etc).

## FEED your model

A way to think about it is we are going to take some set of \[data\] and *feed* or *push* it through our model to get some \[output\]

Example: \[0 and 1s\] --\>

\[complex model with lots of covariates but but focal parameter is a treatment vs control\] --\>

\[estimated means for treatment and control groups\]

## Predicted/fitted values

We often pass *new* data to the model The fitted values in {broom} are taking the original Xs and feeding them into the model, and everyone gets a predicted value (which we can calculate residuals from). But *new* values are often much more helpful.

We already did this with our parent height at 64 example. Same with our dummy variable example where we fed 0 or 1s. Note we did not have to consider our sample size, our model doesn't really care. Just feed it what is relevant.

## How do we feed it?

::: columns
::: {.column width="60%"}
First step is to get a data grid. There are many options. base::expand.grid, tidyr::expand_grid, modelr::data_grid, marginaleffects::datagrid, modelbased::visualization_grid, emmeans::ref_grid

We will discuss two, which I find to be the most user friendly. 1. Modelr, which works well with tidyverse and tidybayes. 2. marginaleffects.
:::

::: {.column width="40%"}
```{r}
#| code-fold: true
library(modelr)
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10))
```
:::
:::

## {marginaleffects}

Like modelr, it can be used for many types of models beyond Bayes. Especially helpful for generalized linear models (logistic etc) https://vincentarelbundock.github.io/marginaleffects/

```{r}
#| code-fold: true
library(marginaleffects)
datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10), model = fit.1)
```

## Feed reference grid to tidybayes

The second step after you have the data/reference grid is to feed it into the model. You may have done something similar using the 'predict' functions of base R or {lme4}. We will use the functions from {marginaleffects} or {tidybayes}, both of which operate similarly. {brms} also has a predict function, but its not as user friendly

------------------------------------------------------------------------

1.  Notice how the predicted values are ".epred"
2.  Notice the number of rows. Where did it come from?

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
 add_epred_draws(fit.1)
```

## Feed reference grid to marginaleffects

1.  Notice how the predicted values are "draw"
2.  Same number of rows. estimate and conf refer to the mean at a fixed parent.c

```{r}
#| code-fold: true
predictions(model = fit.1,
             datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10))) %>% 
  posterior_draws()
```

## plot predictions from tidybayes

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
 add_epred_draws(fit.1) %>% 
    ggplot(aes(x = parent.c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.95), color = "grey") +
  geom_point(data = galton.data, size = 2)
```

## plot predictions from marginaleffects

```{r}
#| code-fold: true
predictions(model = fit.1,
             datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10))) %>% 
  posterior_draws() %>% 
  ggplot(aes(x = parent.c, y = child)) +
  stat_lineribbon(aes(y = draw), .width = c(.95), color = "grey") +
  geom_point(data = galton.data, size = 2)
```

## Plots are easy to tweak

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
 add_epred_draws(fit.1) %>%
  mutate(parent.c = parent.c + 68.31) %>% 
    ggplot(aes(x = parent.c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.95), color = "grey") +
  geom_point(aes(x = parent), data = galton.data)
```

## Confidence Band = many regression lines

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
 add_epred_draws(fit.1, ndraws = 50) %>% 
   mutate(parent.c = parent.c + 68.31) %>% 
  ggplot(aes(x = parent.c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1)
```

## average/marginal effects

We often want to get predicted values for specific combinations of model variables. Like x = 64 inches.

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = -4.3) %>% 
 add_epred_draws(fit.1) %>% 
    ggplot(aes(x = .epred)) +
  stat_halfeye( .width = c(.95))
```

## average/marginal effects

When we are dealing with multiple regression, we sometimes want to get a slope or estimate at different levels of other variables. From {marginalmeans} we can use the slopes function. Note that our slope should be the same at all levels of X.

```{r}
#| code-fold: true
slopes(
  fit.1,
  newdata = datagrid(
    parent.c = -4.3))
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
slopes(fit.1,
  newdata = datagrid(
    parent.c = -4.3)) %>% 
  posterior_draws() %>% 
  ggplot(aes(x = draw)) +
  stat_halfeye( .width = c(.95))
  
```

## Summary

By feeding our model numbers we can accomplish a number of handy things:

-   Individual level predictions (y-hats)
-   Plot/visualize predictions across X\
-   Distribution/Standard errors at certain levels of X\
-   Coefficients/parameters at different levels of X\
-   Post hoc comparisons

## A conceptual framework (from M2M)

One needs to ask:

1.  Quantity: What is the quantity of interest? (predictions, counterfactuals, slopes)
2.  Predictors: What predictor values are we interested in? (By defining a grid, you define what units estimates apply to)
3.  Aggregation: Unit-level, aggregated,subgroup averages, or weighted estimates? (When we compute over a grid of predictor values, we obtain a point estimates for each row of the grid. Do we want each or do we want some summary?)

## Terminology issues

In brms/tidybayes there is a simplification of the above.

1.  fitted/epred (expected prediction) style, Expected value of Y given Xs. Propagates uncertainty in $b_o$ & $b_1$.

2.  prediction style, where we are propagating uncertainty in $b_o$, $b_1$ & $\hat{\sigma}$. What is plausible when we collect a new subject? Predictions, various grids, unit level.

3.  Link transformed. Only useful for generalized linear models (ie do you want predictions in logits or in probabilities?). Changes the type of DV youre interested in.

## predicted values at unit level

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
  add_predicted_draws(fit.1) %>% 
  ggplot(aes(x = parent.c, y = child)) +
     stat_lineribbon(aes(y = .prediction), 
                  .width = .95, 
                  alpha = .5,
                  show.legend = F) +
  geom_point(data = galton.data, size = 2,alpha = .5) 
```

## predicted values

Each .prediction is feeding in a parent.c value we prespecified into our model $\hat{Y}_i = \beta_0 + \beta_1 X + \epsilon_i$. Each row of the posterior has different estimates for each of the three parameters. With epred we were only including 2 parameters as it naturally averages over the uncertainty.

```{r}
#| code-fold: true
galton.data %>% 
data_grid(parent.c = seq_range(parent.c, n = 10)) %>% 
  add_predicted_draws(fit.1)
```

## predicted values

marginaleffects does it a little bit, as the resulting df is more compact. This is helpful when working with complex datasets, though it does result in less opportunity to sanity check.

```{r}
#| code-fold: true
predictions(model = fit.1,
            type = "prediction",
             datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10)))

```

## predicted values

Though if you want to see that, then just use the posterior draws function.

```{r}
#| code-fold: true
predictions(model = fit.1,
            type = "prediction",
             datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10))) %>% 
  posterior_draws()

```

## predicted values

Which allows you to use tidybayes functions

```{r}
#| code-fold: true
predictions(model = fit.1,
            type = "prediction",
             datagrid(parent.c = seq(from = min(galton.data$parent.c), to = max(galton.data$parent.c), length.out = 10))) %>% 
  posterior_draws() %>% 
  ggplot(aes(x = parent.c, y = child)) +
     stat_lineribbon(aes(y = draw), 
                  .width = .95, 
                  alpha = .5,
                  show.legend = F) +
  geom_point(data = galton.data, size = 2,alpha = .5)
```

------------------------------------------------------------------------

predictions function defaults to returning "fitted" values based on full original data as the grid. Note the size of the df is the sample size

```{r}
predictions(fit.1)
```

------------------------------------------------------------------------

Representative, mean grid. Which is the same as just averaging all predictions.

```{r}
predictions(fit.1,  newdata = "mean")
```

```{r}
avg_predictions(fit.1)
```

------------------------------------------------------------------------

Adding new data to make an "interesting" grid. With more complex models you can have more interesting combinations. Right now we have just a single variable

```{r}
predictions(fit.1,  newdata = datagrid(parent.c = c(30, -2)))
```

------------------------------------------------------------------------

Often we have categories and would like a grid across all of these categories. This is described as "crossing" our categorical variables.

```{r, eval = FALSE}

predictions(fit.1,  newdata = datagrid(fac1 = unique, fac2 = unique, covariate = mean)
```

------------------------------------------------------------------------

## predicted values

-   The plotted predictions can show you the potential spread in the cases. As opposed to epred/fitted values which are specific to $\mu$, predicted values serve as simulated new data.

-   If a model is a good fit we should be able to use it to generate data that resemble the data we observed. This is the basis of the posterior predictive distribution and PP checks.

-   This is also what is meant by a `generative` model.

## posterior predictive distribution

::: columns
::: {.column width="40%"}
-   One way to check whether our model is a good is to see if the predictions match up with our observed data. After all, our model is supposed to mimic how the world works.
-   'pp_check' asks: do those simulated samples match the observed?
:::

::: {.column width="60%"}
```{r}
pp_check(fit.1)
```
:::
:::

```{r}
#| code-fold: true
galton.data %>% 
 add_predicted_draws(fit.1) %>% 
    sample_draws(10) %>% 
    ggplot(aes(.prediction, group = .draw)) +
   geom_line(stat = "density", alpha = .1) +
  geom_density(aes(child, group = .draw))+
  theme_classic()

```
